{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce5252b-65d4-4d55-b0f4-40ca0ff8228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required packages:\n",
      "  - faiss-cpu\n",
      "  - sentence-transformers\n",
      "  - PyMuPDF\n",
      "  - numpy\n",
      "  - requests\n",
      "\n",
      "Install with: pip install faiss-cpu sentence-transformers PyMuPDF numpy requests\n",
      "--------------------------------------------------\n",
      "Loading embedding model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7ad946c2fb45b3a40a0d16bfff23cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1ba19a5b284555b6f82bcc4026b113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a107b14c2bc54784b52b53e7e076299b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb00792801f4d54974cde0f89d2f49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7297eedf29d41218aa92de787264748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb335b70b0d641d2b27fad7c4659cb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c0a26d794f4fda867b1166fef42f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871976ccbeda4333ae6c56dd4a26b0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e138272eb947a9932c84fa1ec5bf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04242f8bea1840cbae0bab502a740e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96214ab1a8b74a7aa4e16556e6828348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing index found. Creating new index...\n",
      "=== Setting up Philippine History RAG System ===\n",
      "Loading PDF: PHILIPPINE-HISTORY-SOURCE-BOOK-FINAL-SEP022021.pdf\n",
      "Processing 646 pages...\n",
      "Extracted page 1/646\n",
      "Extracted page 2/646\n",
      "Extracted page 3/646\n",
      "Extracted page 4/646\n",
      "Extracted page 5/646\n",
      "Extracted page 6/646\n",
      "Extracted page 7/646\n",
      "Extracted page 8/646\n",
      "Extracted page 9/646\n",
      "Extracted page 10/646\n",
      "Extracted page 11/646\n",
      "Extracted page 12/646\n",
      "Extracted page 13/646\n",
      "Extracted page 14/646\n",
      "Extracted page 15/646\n",
      "Extracted page 16/646\n",
      "Extracted page 17/646\n",
      "Extracted page 18/646\n",
      "Extracted page 19/646\n",
      "Extracted page 20/646\n",
      "Extracted page 21/646\n",
      "Extracted page 22/646\n",
      "Extracted page 23/646\n",
      "Extracted page 24/646\n",
      "Extracted page 25/646\n",
      "Extracted page 26/646\n",
      "Extracted page 27/646\n",
      "Extracted page 28/646\n",
      "Extracted page 29/646\n",
      "Extracted page 30/646\n",
      "Extracted page 31/646\n",
      "Extracted page 32/646\n",
      "Extracted page 33/646\n",
      "Extracted page 34/646\n",
      "Extracted page 35/646\n",
      "Extracted page 36/646\n",
      "Extracted page 37/646\n",
      "Extracted page 38/646\n",
      "Extracted page 39/646\n",
      "Extracted page 40/646\n",
      "Extracted page 41/646\n",
      "Extracted page 42/646\n",
      "Extracted page 43/646\n",
      "Extracted page 44/646\n",
      "Extracted page 45/646\n",
      "Extracted page 46/646\n",
      "Extracted page 47/646\n",
      "Extracted page 48/646\n",
      "Extracted page 49/646\n",
      "Extracted page 50/646\n",
      "Extracted page 51/646\n",
      "Extracted page 52/646\n",
      "Extracted page 53/646\n",
      "Extracted page 54/646\n",
      "Extracted page 55/646\n",
      "Extracted page 56/646\n",
      "Extracted page 57/646\n",
      "Extracted page 58/646\n",
      "Extracted page 59/646\n",
      "Extracted page 60/646\n",
      "Extracted page 61/646\n",
      "Extracted page 62/646\n",
      "Extracted page 63/646\n",
      "Extracted page 64/646\n",
      "Extracted page 65/646\n",
      "Extracted page 66/646\n",
      "Extracted page 67/646\n",
      "Extracted page 68/646\n",
      "Extracted page 69/646\n",
      "Extracted page 70/646\n",
      "Extracted page 71/646\n",
      "Extracted page 72/646\n",
      "Extracted page 73/646\n",
      "Extracted page 74/646\n",
      "Extracted page 75/646\n",
      "Extracted page 76/646\n",
      "Extracted page 77/646\n",
      "Extracted page 78/646\n",
      "Extracted page 79/646\n",
      "Extracted page 80/646\n",
      "Extracted page 81/646\n",
      "Extracted page 82/646\n",
      "Extracted page 83/646\n",
      "Extracted page 84/646\n",
      "Extracted page 85/646\n",
      "Extracted page 86/646\n",
      "Extracted page 87/646\n",
      "Extracted page 88/646\n",
      "Extracted page 89/646\n",
      "Extracted page 90/646\n",
      "Extracted page 91/646\n",
      "Extracted page 92/646\n",
      "Extracted page 93/646\n",
      "Extracted page 94/646\n",
      "Extracted page 95/646\n",
      "Extracted page 96/646\n",
      "Extracted page 97/646\n",
      "Extracted page 98/646\n",
      "Extracted page 99/646\n",
      "Extracted page 100/646\n",
      "Extracted page 101/646\n",
      "Extracted page 102/646\n",
      "Extracted page 103/646\n",
      "Extracted page 104/646\n",
      "Extracted page 105/646\n",
      "Extracted page 106/646\n",
      "Extracted page 107/646\n",
      "Extracted page 108/646\n",
      "Extracted page 109/646\n",
      "Extracted page 110/646\n",
      "Extracted page 111/646\n",
      "Extracted page 112/646\n",
      "Extracted page 113/646\n",
      "Extracted page 114/646\n",
      "Extracted page 115/646\n",
      "Extracted page 116/646\n",
      "Extracted page 117/646\n",
      "Extracted page 118/646\n",
      "Extracted page 119/646\n",
      "Extracted page 120/646\n",
      "Extracted page 121/646\n",
      "Extracted page 122/646\n",
      "Extracted page 123/646\n",
      "Extracted page 124/646\n",
      "Extracted page 125/646\n",
      "Extracted page 126/646\n",
      "Extracted page 127/646\n",
      "Extracted page 128/646\n",
      "Extracted page 129/646\n",
      "Extracted page 130/646\n",
      "Extracted page 131/646\n",
      "Extracted page 132/646\n",
      "Extracted page 133/646\n",
      "Extracted page 134/646\n",
      "Extracted page 135/646\n",
      "Extracted page 136/646\n",
      "Extracted page 137/646\n",
      "Extracted page 138/646\n",
      "Extracted page 139/646\n",
      "Extracted page 140/646\n",
      "Extracted page 141/646\n",
      "Extracted page 142/646\n",
      "Extracted page 143/646\n",
      "Extracted page 144/646\n",
      "Extracted page 145/646\n",
      "Extracted page 146/646\n",
      "Extracted page 147/646\n",
      "Extracted page 148/646\n",
      "Extracted page 149/646\n",
      "Extracted page 150/646\n",
      "Extracted page 151/646\n",
      "Extracted page 152/646\n",
      "Extracted page 153/646\n",
      "Extracted page 154/646\n",
      "Extracted page 155/646\n",
      "Extracted page 156/646\n",
      "Extracted page 157/646\n",
      "Extracted page 158/646\n",
      "Extracted page 159/646\n",
      "Extracted page 160/646\n",
      "Extracted page 161/646\n",
      "Extracted page 162/646\n",
      "Extracted page 163/646\n",
      "Extracted page 164/646\n",
      "Extracted page 165/646\n",
      "Extracted page 166/646\n",
      "Extracted page 167/646\n",
      "Extracted page 168/646\n",
      "Extracted page 169/646\n",
      "Extracted page 170/646\n",
      "Extracted page 171/646\n",
      "Extracted page 172/646\n",
      "Extracted page 173/646\n",
      "Extracted page 174/646\n",
      "Extracted page 175/646\n",
      "Extracted page 176/646\n",
      "Extracted page 177/646\n",
      "Extracted page 178/646\n",
      "Extracted page 179/646\n",
      "Extracted page 180/646\n",
      "Extracted page 181/646\n",
      "Extracted page 182/646\n",
      "Extracted page 183/646\n",
      "Extracted page 184/646\n",
      "Extracted page 185/646\n",
      "Extracted page 186/646\n",
      "Extracted page 187/646\n",
      "Extracted page 188/646\n",
      "Extracted page 189/646\n",
      "Extracted page 190/646\n",
      "Extracted page 191/646\n",
      "Extracted page 192/646\n",
      "Extracted page 193/646\n",
      "Extracted page 194/646\n",
      "Extracted page 195/646\n",
      "Extracted page 196/646\n",
      "Extracted page 197/646\n",
      "Extracted page 198/646\n",
      "Extracted page 199/646\n",
      "Extracted page 200/646\n",
      "Extracted page 201/646\n",
      "Extracted page 202/646\n",
      "Extracted page 203/646\n",
      "Extracted page 204/646\n",
      "Extracted page 205/646\n",
      "Extracted page 206/646\n",
      "Extracted page 207/646\n",
      "Extracted page 208/646\n",
      "Extracted page 209/646\n",
      "Extracted page 210/646\n",
      "Extracted page 211/646\n",
      "Extracted page 212/646\n",
      "Extracted page 213/646\n",
      "Extracted page 214/646\n",
      "Extracted page 215/646\n",
      "Extracted page 216/646\n",
      "Extracted page 217/646\n",
      "Extracted page 218/646\n",
      "Extracted page 219/646\n",
      "Extracted page 220/646\n",
      "Extracted page 221/646\n",
      "Extracted page 222/646\n",
      "Extracted page 223/646\n",
      "Extracted page 224/646\n",
      "Extracted page 225/646\n",
      "Extracted page 226/646\n",
      "Extracted page 227/646\n",
      "Extracted page 228/646\n",
      "Extracted page 229/646\n",
      "Extracted page 230/646\n",
      "Extracted page 231/646\n",
      "Extracted page 232/646\n",
      "Extracted page 233/646\n",
      "Extracted page 234/646\n",
      "Extracted page 235/646\n",
      "Extracted page 236/646\n",
      "Extracted page 237/646\n",
      "Extracted page 238/646\n",
      "Extracted page 239/646\n",
      "Extracted page 240/646\n",
      "Extracted page 241/646\n",
      "Extracted page 242/646\n",
      "Extracted page 243/646\n",
      "Extracted page 244/646\n",
      "Extracted page 245/646\n",
      "Extracted page 246/646\n",
      "Extracted page 247/646\n",
      "Extracted page 248/646\n",
      "Extracted page 249/646\n",
      "Extracted page 250/646\n",
      "Extracted page 251/646\n",
      "Extracted page 252/646\n",
      "Extracted page 253/646\n",
      "Extracted page 254/646\n",
      "Extracted page 255/646\n",
      "Extracted page 256/646\n",
      "Extracted page 257/646\n",
      "Extracted page 258/646\n",
      "Extracted page 259/646\n",
      "Extracted page 260/646\n",
      "Extracted page 261/646\n",
      "Extracted page 262/646\n",
      "Extracted page 263/646\n",
      "Extracted page 264/646\n",
      "Extracted page 265/646\n",
      "Extracted page 266/646\n",
      "Extracted page 267/646\n",
      "Extracted page 268/646\n",
      "Extracted page 269/646\n",
      "Extracted page 270/646\n",
      "Extracted page 271/646\n",
      "Extracted page 272/646\n",
      "Extracted page 273/646\n",
      "Extracted page 274/646\n",
      "Extracted page 275/646\n",
      "Extracted page 276/646\n",
      "Extracted page 277/646\n",
      "Extracted page 278/646\n",
      "Extracted page 279/646\n",
      "Extracted page 280/646\n",
      "Extracted page 281/646\n",
      "Extracted page 282/646\n",
      "Extracted page 283/646\n",
      "Extracted page 284/646\n",
      "Extracted page 285/646\n",
      "Extracted page 286/646\n",
      "Extracted page 287/646\n",
      "Extracted page 288/646\n",
      "Extracted page 289/646\n",
      "Extracted page 290/646\n",
      "Extracted page 291/646\n",
      "Extracted page 292/646\n",
      "Extracted page 293/646\n",
      "Extracted page 294/646\n",
      "Extracted page 295/646\n",
      "Extracted page 296/646\n",
      "Extracted page 297/646\n",
      "Extracted page 298/646\n",
      "Extracted page 299/646\n",
      "Extracted page 300/646\n",
      "Extracted page 301/646\n",
      "Extracted page 302/646\n",
      "Extracted page 303/646\n",
      "Extracted page 304/646\n",
      "Extracted page 305/646\n",
      "Extracted page 306/646\n",
      "Extracted page 307/646\n",
      "Extracted page 308/646\n",
      "Extracted page 309/646\n",
      "Extracted page 310/646\n",
      "Extracted page 311/646\n",
      "Extracted page 312/646\n",
      "Extracted page 313/646\n",
      "Extracted page 314/646\n",
      "Extracted page 315/646\n",
      "Extracted page 316/646\n",
      "Extracted page 317/646\n",
      "Extracted page 318/646\n",
      "Extracted page 319/646\n",
      "Extracted page 320/646\n",
      "Extracted page 321/646\n",
      "Extracted page 322/646\n",
      "Extracted page 323/646\n",
      "Extracted page 324/646\n",
      "Extracted page 325/646\n",
      "Extracted page 326/646\n",
      "Extracted page 327/646\n",
      "Extracted page 328/646\n",
      "Extracted page 329/646\n",
      "Extracted page 330/646\n",
      "Extracted page 331/646\n",
      "Extracted page 332/646\n",
      "Extracted page 333/646\n",
      "Extracted page 334/646\n",
      "Extracted page 335/646\n",
      "Extracted page 336/646\n",
      "Extracted page 337/646\n",
      "Extracted page 338/646\n",
      "Extracted page 339/646\n",
      "Extracted page 340/646\n",
      "Extracted page 341/646\n",
      "Extracted page 342/646\n",
      "Extracted page 343/646\n",
      "Extracted page 344/646\n",
      "Extracted page 345/646\n",
      "Extracted page 346/646\n",
      "Extracted page 347/646\n",
      "Extracted page 348/646\n",
      "Extracted page 349/646\n",
      "Extracted page 350/646\n",
      "Extracted page 351/646\n",
      "Extracted page 352/646\n",
      "Extracted page 353/646\n",
      "Extracted page 354/646\n",
      "Extracted page 355/646\n",
      "Extracted page 356/646\n",
      "Extracted page 357/646\n",
      "Extracted page 358/646\n",
      "Extracted page 359/646\n",
      "Extracted page 360/646\n",
      "Extracted page 361/646\n",
      "Extracted page 362/646\n",
      "Extracted page 363/646\n",
      "Extracted page 364/646\n",
      "Extracted page 365/646\n",
      "Extracted page 366/646\n",
      "Extracted page 367/646\n",
      "Extracted page 368/646\n",
      "Extracted page 369/646\n",
      "Extracted page 370/646\n",
      "Extracted page 371/646\n",
      "Extracted page 372/646\n",
      "Extracted page 373/646\n",
      "Extracted page 374/646\n",
      "Extracted page 375/646\n",
      "Extracted page 376/646\n",
      "Extracted page 377/646\n",
      "Extracted page 378/646\n",
      "Extracted page 379/646\n",
      "Extracted page 380/646\n",
      "Extracted page 381/646\n",
      "Extracted page 382/646\n",
      "Extracted page 383/646\n",
      "Extracted page 384/646\n",
      "Extracted page 385/646\n",
      "Extracted page 386/646\n",
      "Extracted page 387/646\n",
      "Extracted page 388/646\n",
      "Extracted page 389/646\n",
      "Extracted page 390/646\n",
      "Extracted page 391/646\n",
      "Extracted page 392/646\n",
      "Extracted page 393/646\n",
      "Extracted page 394/646\n",
      "Extracted page 395/646\n",
      "Extracted page 396/646\n",
      "Extracted page 397/646\n",
      "Extracted page 398/646\n",
      "Extracted page 399/646\n",
      "Extracted page 400/646\n",
      "Extracted page 401/646\n",
      "Extracted page 402/646\n",
      "Extracted page 403/646\n",
      "Extracted page 404/646\n",
      "Extracted page 405/646\n",
      "Extracted page 406/646\n",
      "Extracted page 407/646\n",
      "Extracted page 408/646\n",
      "Extracted page 409/646\n",
      "Extracted page 410/646\n",
      "Extracted page 411/646\n",
      "Extracted page 412/646\n",
      "Extracted page 413/646\n",
      "Extracted page 414/646\n",
      "Extracted page 415/646\n",
      "Extracted page 416/646\n",
      "Extracted page 417/646\n",
      "Extracted page 418/646\n",
      "Extracted page 419/646\n",
      "Extracted page 420/646\n",
      "Extracted page 421/646\n",
      "Extracted page 422/646\n",
      "Extracted page 423/646\n",
      "Extracted page 424/646\n",
      "Extracted page 425/646\n",
      "Extracted page 426/646\n",
      "Extracted page 427/646\n",
      "Extracted page 428/646\n",
      "Extracted page 429/646\n",
      "Extracted page 430/646\n",
      "Extracted page 431/646\n",
      "Extracted page 432/646\n",
      "Extracted page 433/646\n",
      "Extracted page 434/646\n",
      "Extracted page 435/646\n",
      "Extracted page 436/646\n",
      "Extracted page 437/646\n",
      "Extracted page 438/646\n",
      "Extracted page 439/646\n",
      "Extracted page 440/646\n",
      "Extracted page 441/646\n",
      "Extracted page 442/646\n",
      "Extracted page 443/646\n",
      "Extracted page 444/646\n",
      "Extracted page 445/646\n",
      "Extracted page 446/646\n",
      "Extracted page 447/646\n",
      "Extracted page 448/646\n",
      "Extracted page 449/646\n",
      "Extracted page 450/646\n",
      "Extracted page 451/646\n",
      "Extracted page 452/646\n",
      "Extracted page 453/646\n",
      "Extracted page 454/646\n",
      "Extracted page 455/646\n",
      "Extracted page 456/646\n",
      "Extracted page 457/646\n",
      "Extracted page 458/646\n",
      "Extracted page 459/646\n",
      "Extracted page 460/646\n",
      "Extracted page 461/646\n",
      "Extracted page 462/646\n",
      "Extracted page 463/646\n",
      "Extracted page 464/646\n",
      "Extracted page 465/646\n",
      "Extracted page 466/646\n",
      "Extracted page 467/646\n",
      "Extracted page 468/646\n",
      "Extracted page 469/646\n",
      "Extracted page 470/646\n",
      "Extracted page 471/646\n",
      "Extracted page 472/646\n",
      "Extracted page 473/646\n",
      "Extracted page 474/646\n",
      "Extracted page 475/646\n",
      "Extracted page 476/646\n",
      "Extracted page 477/646\n",
      "Extracted page 478/646\n",
      "Extracted page 479/646\n",
      "Extracted page 480/646\n",
      "Extracted page 481/646\n",
      "Extracted page 482/646\n",
      "Extracted page 483/646\n",
      "Extracted page 484/646\n",
      "Extracted page 485/646\n",
      "Extracted page 486/646\n",
      "Extracted page 487/646\n",
      "Extracted page 488/646\n",
      "Extracted page 489/646\n",
      "Extracted page 490/646\n",
      "Extracted page 491/646\n",
      "Extracted page 492/646\n",
      "Extracted page 493/646\n",
      "Extracted page 494/646\n",
      "Extracted page 495/646\n",
      "Extracted page 496/646\n",
      "Extracted page 497/646\n",
      "Extracted page 498/646\n",
      "Extracted page 499/646\n",
      "Extracted page 500/646\n",
      "Extracted page 501/646\n",
      "Extracted page 502/646\n",
      "Extracted page 503/646\n",
      "Extracted page 504/646\n",
      "Extracted page 505/646\n",
      "Extracted page 506/646\n",
      "Extracted page 507/646\n",
      "Extracted page 508/646\n",
      "Extracted page 509/646\n",
      "Extracted page 510/646\n",
      "Extracted page 511/646\n",
      "Extracted page 512/646\n",
      "Extracted page 513/646\n",
      "Extracted page 514/646\n",
      "Extracted page 515/646\n",
      "Extracted page 516/646\n",
      "Extracted page 517/646\n",
      "Extracted page 518/646\n",
      "Extracted page 519/646\n",
      "Extracted page 520/646\n",
      "Extracted page 521/646\n",
      "Extracted page 522/646\n",
      "Extracted page 523/646\n",
      "Extracted page 524/646\n",
      "Extracted page 525/646\n",
      "Extracted page 526/646\n",
      "Extracted page 527/646\n",
      "Extracted page 528/646\n",
      "Extracted page 529/646\n",
      "Extracted page 530/646\n",
      "Extracted page 531/646\n",
      "Extracted page 532/646\n",
      "Extracted page 533/646\n",
      "Extracted page 534/646\n",
      "Extracted page 535/646\n",
      "Extracted page 536/646\n",
      "Extracted page 537/646\n",
      "Extracted page 538/646\n",
      "Extracted page 539/646\n",
      "Extracted page 540/646\n",
      "Extracted page 541/646\n",
      "Extracted page 542/646\n",
      "Extracted page 543/646\n",
      "Extracted page 544/646\n",
      "Extracted page 545/646\n",
      "Extracted page 546/646\n",
      "Extracted page 547/646\n",
      "Extracted page 548/646\n",
      "Extracted page 549/646\n",
      "Extracted page 550/646\n",
      "Extracted page 551/646\n",
      "Extracted page 552/646\n",
      "Extracted page 553/646\n",
      "Extracted page 554/646\n",
      "Extracted page 555/646\n",
      "Extracted page 556/646\n",
      "Extracted page 557/646\n",
      "Extracted page 558/646\n",
      "Extracted page 559/646\n",
      "Extracted page 560/646\n",
      "Extracted page 561/646\n",
      "Extracted page 562/646\n",
      "Extracted page 563/646\n",
      "Extracted page 564/646\n",
      "Extracted page 565/646\n",
      "Extracted page 566/646\n",
      "Extracted page 567/646\n",
      "Extracted page 568/646\n",
      "Extracted page 569/646\n",
      "Extracted page 570/646\n",
      "Extracted page 571/646\n",
      "Extracted page 572/646\n",
      "Extracted page 573/646\n",
      "Extracted page 574/646\n",
      "Extracted page 575/646\n",
      "Extracted page 576/646\n",
      "Extracted page 577/646\n",
      "Extracted page 578/646\n",
      "Extracted page 579/646\n",
      "Extracted page 580/646\n",
      "Extracted page 581/646\n",
      "Extracted page 582/646\n",
      "Extracted page 583/646\n",
      "Extracted page 584/646\n",
      "Extracted page 585/646\n",
      "Extracted page 586/646\n",
      "Extracted page 587/646\n",
      "Extracted page 588/646\n",
      "Extracted page 589/646\n",
      "Extracted page 590/646\n",
      "Extracted page 591/646\n",
      "Extracted page 592/646\n",
      "Extracted page 593/646\n",
      "Extracted page 594/646\n",
      "Extracted page 595/646\n",
      "Extracted page 596/646\n",
      "Extracted page 597/646\n",
      "Extracted page 598/646\n",
      "Extracted page 599/646\n",
      "Extracted page 600/646\n",
      "Extracted page 601/646\n",
      "Extracted page 602/646\n",
      "Extracted page 603/646\n",
      "Extracted page 604/646\n",
      "Extracted page 605/646\n",
      "Extracted page 606/646\n",
      "Extracted page 607/646\n",
      "Extracted page 608/646\n",
      "Extracted page 609/646\n",
      "Extracted page 610/646\n",
      "Extracted page 611/646\n",
      "Extracted page 612/646\n",
      "Extracted page 613/646\n",
      "Extracted page 614/646\n",
      "Extracted page 615/646\n",
      "Extracted page 616/646\n",
      "Extracted page 617/646\n",
      "Extracted page 618/646\n",
      "Extracted page 619/646\n",
      "Extracted page 620/646\n",
      "Extracted page 621/646\n",
      "Extracted page 622/646\n",
      "Extracted page 623/646\n",
      "Extracted page 624/646\n",
      "Extracted page 625/646\n",
      "Extracted page 626/646\n",
      "Extracted page 627/646\n",
      "Extracted page 628/646\n",
      "Extracted page 629/646\n",
      "Extracted page 630/646\n",
      "Extracted page 631/646\n",
      "Extracted page 632/646\n",
      "Extracted page 633/646\n",
      "Extracted page 634/646\n",
      "Extracted page 635/646\n",
      "Extracted page 636/646\n",
      "Extracted page 637/646\n",
      "Extracted page 638/646\n",
      "Extracted page 639/646\n",
      "Extracted page 640/646\n",
      "Extracted page 641/646\n",
      "Extracted page 642/646\n",
      "Extracted page 643/646\n",
      "Extracted page 644/646\n",
      "Extracted page 645/646\n",
      "Extracted page 646/646\n",
      "Successfully extracted 1582909 characters from PDF\n",
      "Creating text chunks...\n",
      "Created 4061 chunks\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bada61b8344eb485c8cd241509d052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings with shape: (4061, 384)\n",
      "Building FAISS index...\n",
      "FAISS index built with 4061 vectors\n",
      "=== Setup complete! ===\n",
      "\n",
      "Index saved to faiss_index.index, chunks saved to chunks.pkl\n",
      "Philippine History RAG System is ready!\n",
      "\n",
      "Sample queries you can try:\n",
      "1. When did the EDSA People Power Revolution happen?\n",
      "2. Who was Jose Rizal?\n",
      "3. What happened during the Spanish colonization of the Philippines?\n",
      "4. Tell me about Ferdinand Marcos and Martial Law\n",
      "5. What was the Katipunan?\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question about Philippine history (or 'quit' to exit):  When did the EDSA People Power Revolution happen?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: When did the EDSA People Power Revolution happen?\n",
      "--------------------------------------------------\n",
      "Retrieving answer from Ollama...\n",
      "============================================================\n",
      "PHILIPPINE HISTORY RAG SYSTEM - QUERY RESULT\n",
      "============================================================\n",
      "Query: When did the EDSA People Power Revolution happen?\n",
      "------------------------------------------------------------\n",
      "ANSWER:\n",
      "February 25, 1986.\n",
      "------------------------------------------------------------\n",
      "RETRIEVED CONTEXT (3 chunks):\n",
      "\n",
      "[Chunk 1 - Score: 0.7608]\n",
      "itarianism supplanted the democratic institutions. On February 25, 1986, after four days of bloodless EDSA People Power Revolution, Marcos was compelled to step down and flee with his family to Hawaii...\n",
      "\n",
      "[Chunk 2 - Score: 0.8177]\n",
      "itarianism supplanted the democratic institutions. On February 25, 1986, after four days of bloodless EDSA People Power Revolution, Marcos was compelled to step down and flee with his family to Hawaii...\n",
      "\n",
      "[Chunk 3 - Score: 0.8408]\n",
      "72, following a spate of bombings in Metro Manila. Constitutional authoritarianism supplanted the democratic institutions. On February 25, 1986, after four days of bloodless EDSA People Power Revoluti...\n",
      "============================================================\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question about Philippine history (or 'quit' to exit):  \"Who is José Rizal and why is he important?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: \"Who is José Rizal and why is he important?\n",
      "--------------------------------------------------\n",
      "Retrieving answer from Ollama...\n",
      "============================================================\n",
      "PHILIPPINE HISTORY RAG SYSTEM - QUERY RESULT\n",
      "============================================================\n",
      "Query: \"Who is José Rizal and why is he important?\n",
      "------------------------------------------------------------\n",
      "ANSWER:\n",
      "José Rizal was a Filipino nationalist who lived during the late 19th century. He is important because he highlighted the problems of the Philippines, identified enemies of his country, and advocated for effective solutions to overcome them, embodying a positive nationalism that inspired Filipinos to strive for security, prosperity, and happiness for their nation.\n",
      "------------------------------------------------------------\n",
      "RETRIEVED CONTEXT (3 chunks):\n",
      "\n",
      "[Chunk 1 - Score: 0.5460]\n",
      "y in which to seek the answer. How did Rizal live? Perhaps we should first ask What did he live for? Rizal lived for his country. How did he live for his country? He lived for it first by understandin...\n",
      "\n",
      "[Chunk 2 - Score: 0.5850]\n",
      "day and the importance of  living for our country. He highlighted the idea of  positive nationalism  that Rizal showed by understanding the social problem and presenting solutions to these challenges.\n",
      "\n",
      "[Chunk 3 - Score: 0.5987]\n",
      ". Such was the positive spirit of his nationalism. Indeed, in every period of a country s history, it is true nationalism that gives its people that necessary energy with which to conquer the problems...\n",
      "============================================================\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question about Philippine history (or 'quit' to exit):  Tell me about the Spanish colonization of the Philippines.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: Tell me about the Spanish colonization of the Philippines.\n",
      "--------------------------------------------------\n",
      "Retrieving answer from Ollama...\n",
      "============================================================\n",
      "PHILIPPINE HISTORY RAG SYSTEM - QUERY RESULT\n",
      "============================================================\n",
      "Query: Tell me about the Spanish colonization of the Philippines.\n",
      "------------------------------------------------------------\n",
      "ANSWER:\n",
      "The Spanish colonization of the Philippines began in 1521 with the arrival of European navigators like Magellan, marking the end of Spanish control of the islands. It was a period of intense struggle for freedom for the Filipino people, who migrated to escape oppression.\n",
      "------------------------------------------------------------\n",
      "RETRIEVED CONTEXT (3 chunks):\n",
      "\n",
      "[Chunk 1 - Score: 0.5356]\n",
      "ending the Spanish control of the islands in 1898. The set of materials included here presents the various facets of colonial life in the nineteenth century Philippines. These were written by various ...\n",
      "\n",
      "[Chunk 2 - Score: 0.6715]\n",
      "d the Arts. 2001 Hernandez, Jose Rhommel B. trans. An Ethnographic Description of the Peoples of Luzon by Fr. Juan Ferrando, O.P. in Colloquia Manilana, Vol. 2 (1994): 85-100  Insurrections by Filipin...\n",
      "\n",
      "[Chunk 3 - Score: 0.6817]\n",
      "come firmer and stronger with the passing of time. The history of the Filipino people is an epic of an unrelenting struggle for freedom. Centuries ago, our forebears sailed in frail vintas to settle i...\n",
      "============================================================\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question about Philippine history (or 'quit' to exit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using the Philippine History RAG system!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Dict, Any\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz  # PyMuPDF\n",
    "from io import BytesIO\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "class PhilippineHistoryRAG:\n",
    "    def __init__(self, \n",
    "                 pdf_path: str = \"philippine_history.pdf\",\n",
    "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 ollama_model: str = \"llama2\",\n",
    "                 ollama_url: str = \"http://localhost:11434\",\n",
    "                 chunk_size: int = 500,\n",
    "                 chunk_overlap: int = 50):\n",
    "        \"\"\"\n",
    "        Initialize the Philippine History RAG system.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the Philippine history PDF file\n",
    "            embedding_model: Name of the sentence transformer model for embeddings\n",
    "            ollama_model: Name of the Ollama model to use\n",
    "            ollama_url: Base URL for Ollama API\n",
    "            chunk_size: Size of text chunks in characters\n",
    "            chunk_overlap: Overlap between chunks in characters\n",
    "        \"\"\"\n",
    "        self.pdf_path = pdf_path\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self.ollama_model = ollama_model\n",
    "        self.ollama_url = ollama_url\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # Initialize components\n",
    "        print(\"Loading embedding model...\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        # Storage for chunks and index\n",
    "        self.chunks = []\n",
    "        self.faiss_index = None\n",
    "        \n",
    "    def load_and_extract_text(self) -> str:\n",
    "        \"\"\"\n",
    "        Load the PDF file and extract text content using PyMuPDF.\n",
    "        \n",
    "        Returns:\n",
    "            Extracted text from the PDF\n",
    "        \"\"\"\n",
    "        print(f\"Loading PDF: {self.pdf_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Open the PDF document\n",
    "            pdf_document = fitz.open(self.pdf_path)\n",
    "            text = \"\"\n",
    "            \n",
    "            print(f\"Processing {len(pdf_document)} pages...\")\n",
    "            \n",
    "            for page_num in range(len(pdf_document)):\n",
    "                page = pdf_document[page_num]\n",
    "                page_text = page.get_text()\n",
    "                text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "                print(f\"Extracted page {page_num + 1}/{len(pdf_document)}\")\n",
    "            \n",
    "            # Close the document\n",
    "            pdf_document.close()\n",
    "            \n",
    "            print(f\"Successfully extracted {len(text)} characters from PDF\")\n",
    "            return text\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"PDF file not found: {self.pdf_path}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error reading PDF: {str(e)}\")\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and preprocess the extracted text.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text from PDF\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,;:!?()-]', ' ', text)\n",
    "        \n",
    "        # Remove multiple consecutive punctuation\n",
    "        text = re.sub(r'[.,;:!?]{2,}', '.', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def create_chunks(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Split text into overlapping chunks.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to chunk\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with text and metadata\n",
    "        \"\"\"\n",
    "        print(\"Creating text chunks...\")\n",
    "        \n",
    "        cleaned_text = self.clean_text(text)\n",
    "        chunks = []\n",
    "        \n",
    "        # Split by sentences first for better chunk boundaries\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', cleaned_text)\n",
    "        \n",
    "        current_chunk = \"\"\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # If adding this sentence would exceed chunk size, save current chunk\n",
    "            if len(current_chunk) + len(sentence) > self.chunk_size and current_chunk:\n",
    "                chunks.append({\n",
    "                    'id': chunk_id,\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'length': len(current_chunk)\n",
    "                })\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                overlap_text = current_chunk[-self.chunk_overlap:] if len(current_chunk) > self.chunk_overlap else current_chunk\n",
    "                current_chunk = overlap_text + \" \" + sentence\n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "        \n",
    "        # Add the last chunk\n",
    "        if current_chunk:\n",
    "            chunks.append({\n",
    "                'id': chunk_id,\n",
    "                'text': current_chunk.strip(),\n",
    "                'length': len(current_chunk)\n",
    "            })\n",
    "        \n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    \n",
    "    def create_embeddings(self, chunks: List[Dict[str, Any]]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create embeddings for all chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of text chunks\n",
    "            \n",
    "        Returns:\n",
    "            Numpy array of embeddings\n",
    "        \"\"\"\n",
    "        print(\"Creating embeddings...\")\n",
    "        \n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n",
    "        \n",
    "        print(f\"Created embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def build_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:\n",
    "        \"\"\"\n",
    "        Build FAISS index from embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Array of embeddings\n",
    "            \n",
    "        Returns:\n",
    "            FAISS index\n",
    "        \"\"\"\n",
    "        print(\"Building FAISS index...\")\n",
    "        \n",
    "        # Create a FAISS index (using L2 distance)\n",
    "        index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        \n",
    "        # Add embeddings to index\n",
    "        index.add(embeddings.astype(np.float32))\n",
    "        \n",
    "        print(f\"FAISS index built with {index.ntotal} vectors\")\n",
    "        return index\n",
    "    \n",
    "    def setup_index(self):\n",
    "        \"\"\"\n",
    "        Complete setup: load PDF, create chunks, embeddings, and FAISS index.\n",
    "        \"\"\"\n",
    "        print(\"=== Setting up Philippine History RAG System ===\")\n",
    "        \n",
    "        # Load and process PDF\n",
    "        text = self.load_and_extract_text()\n",
    "        self.chunks = self.create_chunks(text)\n",
    "        \n",
    "        # Create embeddings and FAISS index\n",
    "        embeddings = self.create_embeddings(self.chunks)\n",
    "        self.faiss_index = self.build_faiss_index(embeddings)\n",
    "        \n",
    "        print(\"=== Setup complete! ===\\n\")\n",
    "    \n",
    "    def save_index(self, index_path: str = \"faiss_index.index\", chunks_path: str = \"chunks.pkl\"):\n",
    "        \"\"\"\n",
    "        Save the FAISS index and chunks to disk.\n",
    "        \n",
    "        Args:\n",
    "            index_path: Path to save FAISS index\n",
    "            chunks_path: Path to save chunks\n",
    "        \"\"\"\n",
    "        if self.faiss_index is None:\n",
    "            raise ValueError(\"No index to save. Run setup_index() first.\")\n",
    "            \n",
    "        faiss.write_index(self.faiss_index, index_path)\n",
    "        with open(chunks_path, 'wb') as f:\n",
    "            pickle.dump(self.chunks, f)\n",
    "        print(f\"Index saved to {index_path}, chunks saved to {chunks_path}\")\n",
    "    \n",
    "    def load_index(self, index_path: str = \"faiss_index.index\", chunks_path: str = \"chunks.pkl\"):\n",
    "        \"\"\"\n",
    "        Load the FAISS index and chunks from disk.\n",
    "        \n",
    "        Args:\n",
    "            index_path: Path to load FAISS index\n",
    "            chunks_path: Path to load chunks\n",
    "        \"\"\"\n",
    "        self.faiss_index = faiss.read_index(index_path)\n",
    "        with open(chunks_path, 'rb') as f:\n",
    "            self.chunks = pickle.load(f)\n",
    "        print(f\"Index loaded from {index_path}, chunks loaded from {chunks_path}\")\n",
    "    \n",
    "    def query_ollama(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Send a query to Ollama and get response.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to send to Ollama\n",
    "            \n",
    "        Returns:\n",
    "            Response from Ollama\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.ollama_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.ollama_model,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False\n",
    "                },\n",
    "                timeout=120\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()['response']\n",
    "            else:\n",
    "                return f\"Error: HTTP {response.status_code} - {response.text}\"\n",
    "                \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            return \"Error: Could not connect to Ollama. Make sure Ollama is running on localhost:11434\"\n",
    "        except requests.exceptions.Timeout:\n",
    "            return \"Error: Request to Ollama timed out\"\n",
    "        except Exception as e:\n",
    "            return f\"Error communicating with Ollama: {str(e)}\"\n",
    "    \n",
    "    def retrieve_relevant_chunks(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve the most relevant chunks for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            top_k: Number of top chunks to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant chunks with similarity scores\n",
    "        \"\"\"\n",
    "        if self.faiss_index is None:\n",
    "            raise ValueError(\"Index not built. Run setup_index() first.\")\n",
    "        \n",
    "        # Create embedding for the query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Search the index\n",
    "        distances, indices = self.faiss_index.search(query_embedding.astype(np.float32), top_k)\n",
    "        \n",
    "        # Get relevant chunks\n",
    "        relevant_chunks = []\n",
    "        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            if idx < len(self.chunks):  # Valid index\n",
    "                chunk = self.chunks[idx].copy()\n",
    "                chunk['similarity_score'] = float(distance)\n",
    "                chunk['rank'] = i + 1\n",
    "                relevant_chunks.append(chunk)\n",
    "        \n",
    "        return relevant_chunks\n",
    "    \n",
    "    def build_context(self, relevant_chunks: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        Build context string from relevant chunks.\n",
    "        \n",
    "        Args:\n",
    "            relevant_chunks: List of relevant chunks\n",
    "            \n",
    "        Returns:\n",
    "            Formatted context string\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        for chunk in relevant_chunks:\n",
    "            context_parts.append(f\"[Chunk {chunk['rank']}]: {chunk['text']}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def answer_query(self, query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Answer a query using the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            top_k: Number of chunks to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing the answer and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Processing query: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Retrieve relevant chunks\n",
    "        relevant_chunks = self.retrieve_relevant_chunks(query, top_k)\n",
    "        \n",
    "        if not relevant_chunks:\n",
    "            return {\n",
    "                'query': query,\n",
    "                'answer': \"I couldn't find relevant information to answer your question.\",\n",
    "                'relevant_chunks': [],\n",
    "                'context': \"\"\n",
    "            }\n",
    "        \n",
    "        # Build context\n",
    "        context = self.build_context(relevant_chunks)\n",
    "        \n",
    "        # Create prompt for Ollama\n",
    "        prompt = f\"\"\"Based on the following context about Philippine history, please answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "        \n",
    "        print(\"Retrieving answer from Ollama...\")\n",
    "        answer = self.query_ollama(prompt)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'relevant_chunks': relevant_chunks,\n",
    "            'context': context\n",
    "        }\n",
    "    \n",
    "    def print_detailed_response(self, result: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Print a detailed response with context and sources.\n",
    "        \n",
    "        Args:\n",
    "            result: Result dictionary from answer_query\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"PHILIPPINE HISTORY RAG SYSTEM - QUERY RESULT\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Query: {result['query']}\")\n",
    "        print(\"-\" * 60)\n",
    "        print(\"ANSWER:\")\n",
    "        print(result['answer'])\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"RETRIEVED CONTEXT ({len(result['relevant_chunks'])} chunks):\")\n",
    "        \n",
    "        for chunk in result['relevant_chunks']:\n",
    "            print(f\"\\n[Chunk {chunk['rank']} - Score: {chunk['similarity_score']:.4f}]\")\n",
    "            print(chunk['text'][:200] + \"...\" if len(chunk['text']) > 200 else chunk['text'])\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the RAG system.\n",
    "    \"\"\"\n",
    "    # Initialize the RAG system\n",
    "    rag = PhilippineHistoryRAG(\n",
    "        pdf_path=\"PHILIPPINE-HISTORY-SOURCE-BOOK-FINAL-SEP022021.pdf\",\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        ollama_model=\"gemma3:1b\",  # Change this to your preferred Ollama model\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    \n",
    "    # Check if saved index exists\n",
    "    if os.path.exists(\"faiss_index.index\") and os.path.exists(\"chunks.pkl\"):\n",
    "        print(\"Found existing index files. Loading...\")\n",
    "        rag.load_index()\n",
    "    else:\n",
    "        print(\"No existing index found. Creating new index...\")\n",
    "        rag.setup_index()\n",
    "        # Save for future use\n",
    "        rag.save_index()\n",
    "    \n",
    "    # Example queries\n",
    "    sample_queries = [\n",
    "        \"When did the EDSA People Power Revolution happen?\",\n",
    "        \"Who was Jose Rizal?\",\n",
    "        \"What happened during the Spanish colonization of the Philippines?\",\n",
    "        \"Tell me about Ferdinand Marcos and Martial Law\",\n",
    "        \"What was the Katipunan?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Philippine History RAG System is ready!\")\n",
    "    print(\"\\nSample queries you can try:\")\n",
    "    for i, query in enumerate(sample_queries, 1):\n",
    "        print(f\"{i}. {query}\")\n",
    "    \n",
    "    # Interactive query loop\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        user_query = input(\"Enter your question about Philippine history (or 'quit' to exit): \").strip()\n",
    "        \n",
    "        if user_query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Thank you for using the Philippine History RAG system!\")\n",
    "            break\n",
    "        \n",
    "        if not user_query:\n",
    "            print(\"Please enter a valid question.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Get answer\n",
    "            result = rag.answer_query(user_query, top_k=3)\n",
    "            \n",
    "            # Print detailed response\n",
    "            rag.print_detailed_response(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4910a-c10c-45d3-b5b4-88998106d708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72cb43c3-0045-440b-8660-86f89db56197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m==================================================================================== test session starts =====================================================================================\u001b[0m\n",
      "platform linux -- Python 3.13.5, pytest-8.4.1, pluggy-1.6.0 -- /home/aditya/teamified_assessment/bin/python3.13\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/aditya/teamified_assessment\n",
      "plugins: mock-3.14.1, cov-6.2.1, langsmith-0.4.8, anyio-4.9.0\n",
      "collected 24 items                                                                                                                                                                           \u001b[0m\u001b[1m\n",
      "\n",
      "teamified_assessment_unit_tests.py::TestPhilippineHistoryRAGInit::test_init_default_parameters \u001b[32mPASSED\u001b[0m\u001b[33m                                                                                  [  4%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestPDFProcessing::test_load_and_extract_text_success \u001b[31mFAILED\u001b[0m\u001b[31m                                                                                       [  8%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestPDFProcessing::test_load_and_extract_text_file_not_found \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                [ 12%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestPDFProcessing::test_load_and_extract_text_general_exception \u001b[32mPASSED\u001b[0m\u001b[31m                                                                             [ 16%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestTextProcessing::test_clean_text \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                                         [ 20%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestTextProcessing::test_create_chunks_basic \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                                [ 25%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestTextProcessing::test_create_chunks_with_overlap \u001b[31mFAILED\u001b[0m\u001b[31m                                                                                         [ 29%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestTextProcessing::test_create_chunks_empty_text \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                           [ 33%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestEmbeddingsAndIndex::test_create_embeddings \u001b[31mFAILED\u001b[0m\u001b[31m                                                                                              [ 37%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestEmbeddingsAndIndex::test_build_faiss_index \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                              [ 41%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestEmbeddingsAndIndex::test_retrieve_relevant_chunks_no_index \u001b[32mPASSED\u001b[0m\u001b[31m                                                                              [ 45%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestEmbeddingsAndIndex::test_retrieve_relevant_chunks_success \u001b[31mFAILED\u001b[0m\u001b[31m                                                                               [ 50%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestOllamaIntegration::test_query_ollama_success \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                            [ 54%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestOllamaIntegration::test_query_ollama_http_error \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                         [ 58%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestOllamaIntegration::test_query_ollama_connection_error \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                   [ 62%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestOllamaIntegration::test_query_ollama_timeout \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                            [ 66%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestSaveAndLoad::test_save_index_no_index \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                                   [ 70%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestSaveAndLoad::test_save_index_success \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                                    [ 75%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestSaveAndLoad::test_load_index_success \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                                    [ 79%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestFullRAGWorkflow::test_build_context \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                                     [ 83%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestFullRAGWorkflow::test_answer_query_success \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                              [ 87%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestFullRAGWorkflow::test_answer_query_no_relevant_chunks \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                   [ 91%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestFullRAGWorkflow::test_print_detailed_response \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                           [ 95%]\u001b[0m\n",
      "teamified_assessment_unit_tests.py::TestSetupIndex::test_setup_index_complete_workflow \u001b[32mPASSED\u001b[0m\u001b[31m                                                                                          [100%]\u001b[0m\n",
      "\n",
      "========================================================================================== FAILURES ==========================================================================================\n",
      "\u001b[31m\u001b[1m____________________________________________________________________ TestPDFProcessing.test_load_and_extract_text_success ____________________________________________________________________\u001b[0m\n",
      "\n",
      "self = <teamified_assessment.PhilippineHistoryRAG object at 0x70a76676b390>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mload_and_extract_text\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m) -> \u001b[96mstr\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Load the PDF file and extract text content using PyMuPDF.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        Extracted text from the PDF\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading PDF: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mself\u001b[39;49;00m.pdf_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Open the PDF document\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            pdf_document = fitz.open(\u001b[96mself\u001b[39;49;00m.pdf_path)\u001b[90m\u001b[39;49;00m\n",
      "            text = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[96mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mProcessing \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mlen\u001b[39;49;00m(pdf_document)\u001b[33m}\u001b[39;49;00m\u001b[33m pages...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "                                ^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: object of type 'Mock' has no len()\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mteamified_assessment.py\u001b[0m:62: TypeError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "self = <teamified_assessment_unit_tests.TestPDFProcessing object at 0x70a768551090>, rag_instance = <teamified_assessment.PhilippineHistoryRAG object at 0x70a76676b390>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_load_and_extract_text_success\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, rag_instance):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Test successful PDF text extraction.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        mock_pdf = Mock()\u001b[90m\u001b[39;49;00m\n",
      "        mock_page = Mock()\u001b[90m\u001b[39;49;00m\n",
      "        mock_page.get_text.return_value = \u001b[33m\"\u001b[39;49;00m\u001b[33mSample text from page 1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m patch(\u001b[33m'\u001b[39;49;00m\u001b[33mteamified_assessment.fitz.open\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, return_value=mock_pdf):\u001b[90m\u001b[39;49;00m\n",
      ">           result = rag_instance.load_and_extract_text()\u001b[90m\u001b[39;49;00m\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mteamified_assessment_unit_tests.py\u001b[0m:48: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <teamified_assessment.PhilippineHistoryRAG object at 0x70a76676b390>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mload_and_extract_text\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m) -> \u001b[96mstr\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Load the PDF file and extract text content using PyMuPDF.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        Extracted text from the PDF\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading PDF: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mself\u001b[39;49;00m.pdf_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Open the PDF document\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            pdf_document = fitz.open(\u001b[96mself\u001b[39;49;00m.pdf_path)\u001b[90m\u001b[39;49;00m\n",
      "            text = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mProcessing \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mlen\u001b[39;49;00m(pdf_document)\u001b[33m}\u001b[39;49;00m\u001b[33m pages...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m page_num \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(pdf_document)):\u001b[90m\u001b[39;49;00m\n",
      "                page = pdf_document[page_num]\u001b[90m\u001b[39;49;00m\n",
      "                page_text = page.get_text()\u001b[90m\u001b[39;49;00m\n",
      "                text += \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m--- Page \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpage_num\u001b[90m \u001b[39;49;00m+\u001b[90m \u001b[39;49;00m\u001b[94m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m ---\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpage_text\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[96mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mExtracted page \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpage_num\u001b[90m \u001b[39;49;00m+\u001b[90m \u001b[39;49;00m\u001b[94m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mlen\u001b[39;49;00m(pdf_document)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Close the document\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            pdf_document.close()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mSuccessfully extracted \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mlen\u001b[39;49;00m(text)\u001b[33m}\u001b[39;49;00m\u001b[33m characters from PDF\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m text\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mFileNotFoundError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mFileNotFoundError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPDF file not found: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mself\u001b[39;49;00m.pdf_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mexcept\u001b[39;49;00m \u001b[96mException\u001b[39;49;00m \u001b[94mas\u001b[39;49;00m e:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mException\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mError reading PDF: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mstr\u001b[39;49;00m(e)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           Exception: Error reading PDF: object of type 'Mock' has no len()\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mteamified_assessment.py\u001b[0m:79: Exception\n",
      "----------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------\n",
      "Loading embedding model...\n",
      "------------------------------------------------------------------------------------ Captured stdout call ------------------------------------------------------------------------------------\n",
      "Loading PDF: PHILIPPINE-HISTORY-SOURCE-BOOK-FINAL-SEP022021.pdf\n",
      "\u001b[31m\u001b[1m_____________________________________________________________________ TestTextProcessing.test_create_chunks_with_overlap _____________________________________________________________________\u001b[0m\n",
      "\n",
      "self = <teamified_assessment_unit_tests.TestTextProcessing object at 0x70a768520e90>, rag_instance = <teamified_assessment.PhilippineHistoryRAG object at 0x70a7668a6360>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_create_chunks_with_overlap\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, rag_instance):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Test chunking with overlap.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Create text longer than chunk_size (100)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        text = \u001b[33m\"\u001b[39;49;00m\u001b[33mA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m * \u001b[94m150\u001b[39;49;00m + \u001b[33m\"\u001b[39;49;00m\u001b[33m. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[33m\"\u001b[39;49;00m\u001b[33mB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m * \u001b[94m150\u001b[39;49;00m + \u001b[33m\"\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        chunks = rag_instance.create_chunks(text)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(chunks) >= \u001b[94m2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Check that chunks have some overlap\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(chunks) > \u001b[94m1\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# This is a simplified check - in reality overlap might be more complex\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(chunks[\u001b[94m0\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]) <= rag_instance.chunk_size + \u001b[94m50\u001b[39;49;00m  \u001b[90m# some tolerance\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: assert 151 <= (100 + 50)\u001b[0m\n",
      "\u001b[1m\u001b[31mE            +  where 151 = len('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA.')\u001b[0m\n",
      "\u001b[1m\u001b[31mE            +  and   100 = <teamified_assessment.PhilippineHistoryRAG object at 0x70a7668a6360>.chunk_size\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mteamified_assessment_unit_tests.py\u001b[0m:105: AssertionError\n",
      "----------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------\n",
      "Loading embedding model...\n",
      "------------------------------------------------------------------------------------ Captured stdout call ------------------------------------------------------------------------------------\n",
      "Creating text chunks...\n",
      "Created 2 chunks\n",
      "\u001b[31m\u001b[1m_______________________________________________________________________ TestEmbeddingsAndIndex.test_create_embeddings ________________________________________________________________________\u001b[0m\n",
      "\n",
      "self = <teamified_assessment_unit_tests.TestEmbeddingsAndIndex object at 0x70a768550e10>, rag_instance = <teamified_assessment.PhilippineHistoryRAG object at 0x70a766612650>\n",
      "sample_chunks = [{'id': 0, 'length': 16, 'text': 'First chunk text'}, {'id': 1, 'length': 17, 'text': 'Second chunk text'}]\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_create_embeddings\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, rag_instance, sample_chunks):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Test embedding creation.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        mock_embeddings = np.random.rand(\u001b[94m2\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       rag_instance.embedding_model.encode.return_value = mock_embeddings\u001b[90m\u001b[39;49;00m\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'method' object has no attribute 'return_value' and no __dict__ for setting new attributes\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mteamified_assessment_unit_tests.py\u001b[0m:132: AttributeError\n",
      "----------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------\n",
      "Loading embedding model...\n",
      "\u001b[31m\u001b[1m________________________________________________________________ TestEmbeddingsAndIndex.test_retrieve_relevant_chunks_success ________________________________________________________________\u001b[0m\n",
      "\n",
      "self = <teamified_assessment_unit_tests.TestEmbeddingsAndIndex object at 0x70a768521350>, rag_instance = <teamified_assessment.PhilippineHistoryRAG object at 0x70a766581040>\n",
      "sample_chunks = [{'id': 0, 'length': 16, 'text': 'First chunk text'}, {'id': 1, 'length': 17, 'text': 'Second chunk text'}]\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_retrieve_relevant_chunks_success\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, rag_instance, sample_chunks):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Test successful chunk retrieval.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Set up mock index and chunks\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        rag_instance.chunks = sample_chunks\u001b[90m\u001b[39;49;00m\n",
      "        mock_index = Mock()\u001b[90m\u001b[39;49;00m\n",
      "        mock_index.search.return_value = (\u001b[90m\u001b[39;49;00m\n",
      "            np.array([[\u001b[94m0.1\u001b[39;49;00m, \u001b[94m0.3\u001b[39;49;00m]]),  \u001b[90m# distances\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            np.array([[\u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m]])       \u001b[90m# indices\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "        rag_instance.faiss_index = mock_index\u001b[90m\u001b[39;49;00m\n",
      ">       rag_instance.embedding_model.encode.return_value = np.random.rand(\u001b[94m1\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'method' object has no attribute 'return_value' and no __dict__ for setting new attributes\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mteamified_assessment_unit_tests.py\u001b[0m:164: AttributeError\n",
      "----------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------\n",
      "Loading embedding model...\n",
      "\u001b[33m====================================================================================== warnings summary ======================================================================================\u001b[0m\n",
      "lib/python3.13/site-packages/faiss/loader.py:49\n",
      "  /home/aditya/teamified_assessment/lib/python3.13/site-packages/faiss/loader.py:49: DeprecationWarning: numpy.core._multiarray_umath is deprecated and has been renamed to numpy._core._multiarray_umath. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core._multiarray_umath.__cpu_features__.\n",
      "    from numpy.core._multiarray_umath import __cpu_features__\n",
      "\n",
      "<frozen importlib._bootstrap>:488\n",
      "<frozen importlib._bootstrap>:488\n",
      "<frozen importlib._bootstrap>:488\n",
      "  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "\n",
      "<frozen importlib._bootstrap>:488\n",
      "<frozen importlib._bootstrap>:488\n",
      "<frozen importlib._bootstrap>:488\n",
      "  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
      "\n",
      "<frozen importlib._bootstrap>:488\n",
      "<frozen importlib._bootstrap>:488\n",
      "  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[36m\u001b[1m================================================================================== short test summary info ===================================================================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m teamified_assessment_unit_tests.py::\u001b[1mTestPDFProcessing::test_load_and_extract_text_success\u001b[0m - Exception: Error reading PDF: object of type 'Mock' has no len()\n",
      "\u001b[31mFAILED\u001b[0m teamified_assessment_unit_tests.py::\u001b[1mTestTextProcessing::test_create_chunks_with_overlap\u001b[0m - AssertionError: assert 151 <= (100 + 50)\n",
      "\u001b[31mFAILED\u001b[0m teamified_assessment_unit_tests.py::\u001b[1mTestEmbeddingsAndIndex::test_create_embeddings\u001b[0m - AttributeError: 'method' object has no attribute 'return_value' and no __dict__ for setting new attributes\n",
      "\u001b[31mFAILED\u001b[0m teamified_assessment_unit_tests.py::\u001b[1mTestEmbeddingsAndIndex::test_retrieve_relevant_chunks_success\u001b[0m - AttributeError: 'method' object has no attribute 'return_value' and no __dict__ for setting new attributes\n",
      "\u001b[31m==================================================================== \u001b[31m\u001b[1m4 failed\u001b[0m, \u001b[32m20 passed\u001b[0m, \u001b[33m9 warnings\u001b[0m\u001b[31m in 99.29s (0:01:39)\u001b[0m\u001b[31m =====================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest teamified_assessment_unit_tests.py -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc8d20-2e01-4fd9-9d9d-a7a8cca4b533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
